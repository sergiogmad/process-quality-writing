{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11090859",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T20:29:08.618192Z",
     "iopub.status.busy": "2024-01-09T20:29:08.617638Z",
     "iopub.status.idle": "2024-01-09T20:29:09.212730Z",
     "shell.execute_reply": "2024-01-09T20:29:09.211857Z"
    },
    "papermill": {
     "duration": 0.602054,
     "end_time": "2024-01-09T20:29:09.215792",
     "exception": false,
     "start_time": "2024-01-09T20:29:08.613738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64842c33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T20:29:09.221920Z",
     "iopub.status.busy": "2024-01-09T20:29:09.221345Z",
     "iopub.status.idle": "2024-01-09T20:29:09.224632Z",
     "shell.execute_reply": "2024-01-09T20:29:09.224071Z"
    },
    "papermill": {
     "duration": 0.00783,
     "end_time": "2024-01-09T20:29:09.226153",
     "exception": false,
     "start_time": "2024-01-09T20:29:09.218323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = Path('../input/linking-writing-processes-to-writing-quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82e1b744",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T20:29:09.231222Z",
     "iopub.status.busy": "2024-01-09T20:29:09.230886Z",
     "iopub.status.idle": "2024-01-09T20:29:19.494428Z",
     "shell.execute_reply": "2024-01-09T20:29:19.493569Z"
    },
    "papermill": {
     "duration": 10.268171,
     "end_time": "2024-01-09T20:29:19.496504",
     "exception": false,
     "start_time": "2024-01-09T20:29:09.228333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_logs = pd.read_csv(path/'train_logs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7a41295",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T20:29:19.501339Z",
     "iopub.status.busy": "2024-01-09T20:29:19.501120Z",
     "iopub.status.idle": "2024-01-09T20:50:33.082478Z",
     "shell.execute_reply": "2024-01-09T20:50:33.081654Z"
    },
    "papermill": {
     "duration": 1273.587611,
     "end_time": "2024-01-09T20:50:33.086117",
     "exception": false,
     "start_time": "2024-01-09T20:29:19.498506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/3407758908.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  simplified_train_logs.drop_duplicates(inplace=True)\n",
      "/tmp/ipykernel_19/3407758908.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  simplified_train_logs.fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "train_logs['num_events'] = train_logs.groupby('id')['event_id'].transform('last')\n",
    "train_logs['total_time_mins'] = np.round(train_logs.groupby('id')['up_time'].transform('last') / 60000, 1)\n",
    "train_logs['input_chars_aux'] = train_logs.groupby('id')['activity'].transform(lambda x: (x == 'Input').sum())\n",
    "    \n",
    "for _, group in train_logs.groupby('id'):\n",
    "    first_strings_replace_chars = 0\n",
    "    second_strings_replace_chars = 0\n",
    "\n",
    "    for replace_str in group[group.activity == \"Replace\"].text_change.values:\n",
    "        arrow_idx = replace_str.find(' => ')\n",
    "        len_second_str = len(replace_str) - arrow_idx - len(' => ')\n",
    "        second_strings_replace_chars += len_second_str\n",
    "\n",
    "    # Assign accumulated values to the corresponding columns for the group\n",
    "    train_logs.loc[group.index, 'second_strings_replace_chars_aux'] = second_strings_replace_chars\n",
    "\n",
    "train_logs['paste_chars_aux'] = train_logs.groupby('id')['activity'].transform(\n",
    "lambda x: (train_logs.loc[x.index, 'text_change'][x == 'Paste']).str.len().sum()\n",
    ")\n",
    "\n",
    "train_logs['chars_process'] = train_logs.input_chars_aux + train_logs.second_strings_replace_chars_aux +\\\n",
    "                          train_logs.paste_chars_aux\n",
    "train_logs['chars_per_min_process'] = np.round(train_logs.chars_process / train_logs.total_time_mins, 1)\n",
    "\n",
    "train_logs['last_word_count_aux'] = train_logs.groupby('id')['word_count'].transform(lambda x: x.values[-1])\n",
    "train_logs['words_per_min_product'] = np.round(train_logs.last_word_count_aux / train_logs.total_time_mins, 1)\n",
    "\n",
    "for _, group in train_logs.groupby('id'):\n",
    "    input_sentences = group[(group['text_change'].str.contains('[\\.\\;\\?\\!\\:]', regex=True)) &\n",
    "                            (group.activity=='Input')].event_id.count()\n",
    "\n",
    "    remove_cut_sentences = group[(group['text_change'].str.contains('[\\.\\;\\?\\!\\:]', regex=True)) &\n",
    "                            (group.activity=='Remove/Cut')].event_id.count()\n",
    "\n",
    "    train_logs.loc[group.index, 'input_sentences_aux'] = input_sentences\n",
    "    train_logs.loc[group.index, 'remove_cut_sentences_aux'] = remove_cut_sentences\n",
    "\n",
    "\n",
    "train_logs['sentences_per_min_product'] = np.round((train_logs['input_sentences_aux'] - \n",
    "                                            train_logs['remove_cut_sentences_aux']) / train_logs['total_time_mins'],1)\n",
    "\n",
    "for _, group in train_logs.groupby('id'):\n",
    "    iki = group['down_time'] - group['up_time'].shift(1)\n",
    "    filtered_iki = [num for num in iki if num >= 2000]\n",
    "    pause_time = sum(filtered_iki)\n",
    "\n",
    "    # Assign accumulated values to the corresponding columns for the group\n",
    "    train_logs.loc[group.index, 'pause_time_aux'] = pause_time\n",
    "\n",
    "train_logs['pause_time_proportion_perc'] = np.round((100 * train_logs.pause_time_aux) / (60000 * train_logs.total_time_mins), 1)\n",
    "\n",
    "for _, group in train_logs.groupby('id'):\n",
    "    group['word_count_up_aux'] = group.word_count > group.shift().word_count\n",
    "    word_count_up_idxs = group[(group.word_count_up_aux) & (group.text_change=='q')].index\n",
    "\n",
    "    # Look for time when word ended\n",
    "    pause_time_w_in_words, pauses_w_in_words = 0, 0\n",
    "    for idx in word_count_up_idxs:\n",
    "        initial_time = group.at[idx, 'up_time']\n",
    "        slice_f = group[(group.index >= idx + 1) & (group.index <= idx + 10) \n",
    "                        & (group.activity=='Input') & (group.text_change.isin(['.', ',', ';', ':', ' ', '!', '?']))] \n",
    "        if not slice_f.empty:\n",
    "            final_time = slice_f['down_time'].values[0]\n",
    "            pause_time_w_in_words += final_time - initial_time\n",
    "            pauses_w_in_words += 1\n",
    "\n",
    "    # Assign accumulated values to the corresponding columns for the group\n",
    "    train_logs.loc[group.index, 'pause_time_w_in_words_aux'] = pause_time_w_in_words\n",
    "    train_logs.loc[group.index, 'pauses_w_in_words_aux'] = pauses_w_in_words\n",
    "\n",
    "\n",
    "train_logs['mean_pause_length_w_in_words'] = np.round(train_logs.pause_time_w_in_words_aux / train_logs.pauses_w_in_words_aux, 1)\n",
    "\n",
    "simplified_train_logs = train_logs[['num_events', 'chars_per_min_process', \n",
    "                                    'words_per_min_product', 'sentences_per_min_product', \n",
    "                                    'pause_time_proportion_perc', 'mean_pause_length_w_in_words']]\n",
    "\n",
    "simplified_train_logs.drop_duplicates(inplace=True)\n",
    "simplified_train_logs.fillna(0, inplace=True)\n",
    "simplified_train_logs.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f24c81c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T20:50:33.090666Z",
     "iopub.status.busy": "2024-01-09T20:50:33.090401Z",
     "iopub.status.idle": "2024-01-09T20:50:33.094762Z",
     "shell.execute_reply": "2024-01-09T20:50:33.094109Z"
    },
    "papermill": {
     "duration": 0.008394,
     "end_time": "2024-01-09T20:50:33.096299",
     "exception": false,
     "start_time": "2024-01-09T20:50:33.087905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = Path('./simplified_train_logs.pkl')\n",
    "\n",
    "with open(file_path, 'wb') as file:\n",
    "        pickle.dump(simplified_train_logs, file)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 6678907,
     "sourceId": 59291,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30615,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1287.678549,
   "end_time": "2024-01-09T20:50:33.817926",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-09T20:29:06.139377",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
